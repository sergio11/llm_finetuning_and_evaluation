{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Analysis of Pre-Trained T5 Family LLMs: Leveraging Prompt Engineering and Few-Shot Examples for Fine-Tuning"
      ],
      "metadata": {
        "id": "u7N21uRWM6zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rapid advancements in natural language processing (NLP) have underscored **the importance of leveraging pre-trained language models to enhance performance across a variety of tasks.** Among these, the T5 family of models has gained prominence due to its versatility and effectiveness in handling diverse language tasks. **This study focuses on the evaluation and analysis of pre-trained T5 models**, with a particular emphasis on utilizing prompt **engineering and few-shot examples for fine-tuning.**\n",
        "\n",
        "Pre-trained models from the T5 family, including variants like T5-Base, T5-Large, and FLAN-T5, have demonstrated significant capabilities in text generation, question answering, and translation. However, their effectiveness can be further optimized through fine-tuning techniques that adapt these models to specific tasks or domains. Prompt engineering, a method of designing and structuring input prompts, combined with few-shot examples, offers a promising approach to enhance model performance without extensive retraining.\n",
        "\n",
        "In this work, we systematically evaluate various T5 models, investigating how different prompt engineering strategies and few-shot learning setups impact their performance. We aim to identify best practices for fine-tuning pre-trained models to achieve better results in real-world applications. By analyzing the strengths and limitations of each model under different prompt conditions, this study provides valuable insights into optimizing T5-based LLMs for diverse NLP tasks and contributes to the broader understanding of prompt-based fine-tuning methodologies."
      ],
      "metadata": {
        "id": "rAAVoxg1NC5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "jGF3WmeIM5CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Dependencies and Implementing Key Functions for T5 Model Introduction"
      ],
      "metadata": {
        "id": "BvhCJJxH9EWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this section, we focus on the critical initial steps of preparing our environment and defining essential functions for working with the T5 model. This involves installing necessary libraries and implementing two key functions: `load_t5_model` and `generate_response_from_prompt`."
      ],
      "metadata": {
        "id": "Oc28F60a9vhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "Lk7hYXeTCvD2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line `from transformers import T5Tokenizer, T5ForConditionalGeneration` is used to import two essential components from the Transformers library. `T5Tokenizer` is used to **prepare text for processing by converting it into a format the T5 model can understand**. `T5ForConditionalGeneration` refers to the T5 model itself, which is designed for generating text based on the processed input. Together, these imports allow you to utilize the T5 model for various text generation tasks."
      ],
      "metadata": {
        "id": "l_fmnDioDAAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_t5_model(name):\n",
        "  tokenizer_T5 = T5Tokenizer.from_pretrained(name)\n",
        "  model_T5 = T5ForConditionalGeneration.from_pretrained(name, device_map=\"auto\")\n",
        "  return tokenizer_T5, model_T5"
      ],
      "metadata": {
        "id": "og6UxAT9CfY3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `load_t5_model(name)` is designed to load both the **T5 model and its tokenizer**. Here’s a breakdown of its implementation:\n",
        "\n",
        "* `tokenizer_T5 = T5Tokenizer.from_pretrained(\"name\"):` This line initializes the T5 tokenizer using the pre-trained `name` model. The tokenizer is responsible for converting text into a format that the **T5 model can process**.\n",
        "\n",
        "* `model_T5 = T5ForConditionalGeneration.from_pretrained(name, device_map=\"auto\"):` This line loads the T5 model for conditional generation, also using the `name` variant. The `device_map=\"auto\"` argument ensures that the model is placed on the appropriate hardware device (CPU or GPU) based on availability.\n",
        "\n",
        "* `return tokenizer_T5, model_T5:` This line returns both the tokenizer and the model as a tuple.\n",
        "\n",
        "In summary, this function loads and returns a **pre-trained T5 tokenizer** and model, which are ready for text processing and generation tasks."
      ],
      "metadata": {
        "id": "HuvLQEIRDeSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_from_prompt(model, prompt, max_length=100):\n",
        "  tokenizer_T5, model_T5 = load_t5_model(model)\n",
        "  prompt_tokens = tokenizer_T5(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "  outputs = model_T5.generate(prompt_tokens, max_length=max_length)\n",
        "  return tokenizer_T5.decode(outputs[0])"
      ],
      "metadata": {
        "id": "-nz2P5n5BF-F"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function generate_response_from_prompt is used to generate a response to a given text prompt using a **T5 model**. Here's a step-by-step breakdown of its implementation:\n",
        "\n",
        "* `tokenizer_T5, model_FT5 = load_t5_model(model):`\n",
        "\n",
        "This line calls the `load_t5_model` function, which returns both the **T5 tokenizer** (`tokenizer_T5`) and the **T5 model** (`model_T5`). It assumes that **load_t5_model** is designed to return these two components.\n",
        "\n",
        "* `prompt_tokens = tokenizer_T5(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\"):`\n",
        "\n",
        " - This line tokenizes the `prompt` using the **T5 tokenizer**, converting it into a format (tensor) suitable for processing by the model. The `return_tensors=\"pt\"` argument specifies that the output should be a PyTorch tensor.\n",
        "\n",
        " - The tensor is then moved to the GPU `(.to(\"cuda\"))` for faster computation, assuming that CUDA (NVIDIA's parallel computing platform) is available.\n",
        "\n",
        "* `outputs = model_FT5.generate(prompt_tokens, max_length=max_length):`\n",
        "\n",
        "This line generates text based on the tokenized `prompt` using the T5 model (`model_T5`). The `max_length` parameter limits the length of the generated response to the specified value.\n",
        "\n",
        "* `return tokenizer_T5.decode(outputs[0]):`\n",
        "\n",
        "This line decodes the generated tokens back into a human-readable text format using the **T5 tokenizer**. It returns the decoded text as the final output."
      ],
      "metadata": {
        "id": "xARp6lJjD_lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of a Base LLM Without Fine-Tuning: Use of T5 Base"
      ],
      "metadata": {
        "id": "jtjc-VuBGu-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the realm of natural language processing, large language models have set new benchmarks in text comprehension and generation. **The T5-Base model, with 220 million parameters and available on Hugging Face**, has been pre-trained on a broad array of datasets, as detailed in its documentation. However, despite its extensive pre-training, **T5-Base has not been specifically fine-tuned for conversational tasks.**\n",
        "\n",
        "In this section, **we will evaluate the performance of the T5-Base model in conversational contexts to examine its capabilities and limitations in dialogue scenarios.** Our goal is to demonstrate that, while T5-Base has been trained on a diverse set of data, its performance in conversational tasks can be suboptimal due to the lack of fine-tuning tailored for such applications.\n",
        "\n",
        "Throughout this analysis, we will present test cases that illustrate how T5-Base handles conversational interactions and discuss the challenges it faces compared to models optimized for dialogue. We will evaluate aspects such as response coherence, contextual relevance, and the model's ability to maintain a natural conversation flow. Through this evaluation, we aim to highlight areas where the base T5 model may fall short in conversational use cases and suggest potential avenues for enhancing its performance through fine-tuning or additional adjustments."
      ],
      "metadata": {
        "id": "dYgrMc3jHCnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"My name\"\n",
        "generate_response_from_prompt(\"t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ujdg8AuY9vm8",
        "outputId": "e1207f9f-c218-4270-c0f3-5448b304a17d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> <extra_id_0> My name is Yvonne. My name is Yvonne. My name is Yvonne.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our test, we provided the prompt **\"My name\"** to the model in hopes of receiving a meaningful and contextually appropriate response. However, the generated output was:\n",
        "\n",
        "`<pad> <extra_id_0> My name is Yvonne. My name is Yvonne. My name is Yvonne.</s>`\n",
        "\n",
        "This result reveals several key issues with the model’s performance in this instance.\n",
        "\n",
        "* Firstly, the repetition in the `response—\"My name is Yvonne\"` being repeated multiple times—suggests that the model struggled to generate a coherent and varied answer. Instead of providing a concise and relevant response to the prompt, the model repeatedly generated the same phrase. This repetition can be attributed to the model’s **lack of fine-tuning for conversational tasks**, which often involves generating diverse and contextually appropriate responses.\n",
        "\n",
        "* Secondly, the presence of special tokens like `<pad>` and `<extra_id_0>` indicates that the model’s output includes formatting elements that may not contribute directly to the meaningful content. The `<pad>` token is used for padding sequences to ensure uniform input lengths, while `<extra_id_0>` might be a placeholder for additional information. These tokens, while necessary for processing within the model, highlight that the output was not optimized for straightforward, user-facing responses.\n",
        "\n",
        "* Finally, the model’s output ending with `</s>` meaning the end of the generated text. This tokenization detail is part of how the model structures its output but does not impact the actual content.\n",
        "\n",
        "Overall, **the unsatisfactory result underscores that the T5-Base model**, in its base form without fine-tuning, may not be well-suited for generating high-quality, contextually relevant responses in conversational scenarios. It points to the need for more specialized fine-tuning to enhance its ability to handle prompts effectively and generate varied, coherent, and contextually accurate answers."
      ],
      "metadata": {
        "id": "bxosmfzWHgcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Today is\"\n",
        "\n",
        "generate_response_from_prompt(\"t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "veHSibFUILkw",
        "outputId": "81fd2a95-7419-43a0-f52d-94f8be90cdae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> <extra_id_0> Today is the day. Today is the day.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What do you think of Mars?\"\n",
        "generate_response_from_prompt(\"t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QT99A8eXIQ6n",
        "outputId": "cfd7dd49-6997-45d0-c876-bf91a50857d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> <extra_id_0> <extra_id_1> What do you think of Mars?</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Climate change refers to significant and lasting changes in the Earth's climate \\\n",
        "and weather patterns over extended periods. It is primarily driven by human activities, \\\n",
        "especially the burning of fossil fuels such as coal, oil, and natural gas, which release \\\n",
        "greenhouse gases into the atmosphere. These gases, including carbon dioxide and methane, \\\n",
        "trap heat and lead to a warming of the planet. The consequences of climate change include \\\n",
        "rising sea levels, more frequent and severe extreme weather events, and disruptions to ecosystems \\\n",
        "and biodiversity. Addressing climate change requires global efforts to reduce greenhouse gas \\\n",
        "emissions, transition to renewable energy sources, and adapt to the changes that are already occurring.\"\"\"\n",
        "\n",
        "prompt = f\"Summarize: {text}\"\n",
        "\n",
        "generate_response_from_prompt(\"t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "K3NFKNUmAg9N",
        "outputId": "7835cb9b-0695-442e-a348-1e738dbbb6eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> False</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result `\"False\"` from the model when summarizing the climate change text indicates that the **t5-base model** did not generate a meaningful summary. Instead, it returned an incorrect output. This suggests that the model, in its base form, **may not be well-suited for summarization tasks or may require fine-tuning to handle such prompts effectively.**"
      ],
      "metadata": {
        "id": "BmOOA_sXIv7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-Shot Learning Technique\n",
        "Few-shot learning is a method used in machine learning where a model is given a small number of examples to learn from before making predictions or generating responses. This technique is particularly useful for tasks where there isn’t a large amount of labeled data available for training."
      ],
      "metadata": {
        "id": "3cWBOxhqNoSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review1 = \"\"\"Love these plugs, have a few now. We use them to plug in lights and \\\n",
        "set timers to turn them on and off via a phone app. Easy to use and linked to \\\n",
        "the internet and apps. Good value for money.\"\"\"\n",
        "\n",
        "review2 = \"\"\"Tried and tried but could never get them to work right. Too bad \\\n",
        "I'm past my return date or they would have gone back.\"\"\"\n",
        "\n",
        "review3 = \"\"\"A well-sized, reliable smart plug. The app is easy to use and set \\\n",
        "up, and works well. I used them to make several lamps. Everything works fine - \\\n",
        "no problems.\"\"\"\n",
        "\n",
        "review4 = \"\"\"Great little product. Super easy to set up. Didn't even need to use \\\n",
        "the Alexa app to do so. Did it with my echo. Now I use it almost daily to turn on \\\n",
        "a light that was a pain to get to.\"\"\"\n",
        "\n",
        "review5 = \"\"\"If I could give this zero stars I would. Plug wouldn’t connect. I \\\n",
        "had to keep connecting it and finally just gave up and returned it. Customer service \\\n",
        "was a complete waste of time.\"\"\"\n",
        "\n",
        "review6 = \"\"\"The smart plug works as advertised. It’s a bit bulky but it does the job \\\n",
        "well. The setup was straightforward, and it integrates seamlessly with my home automation system.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Review: {review1}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review2}\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: {review3}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review4}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review5}\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: {review6}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Uqs_7wgcKzyF",
        "outputId": "f64621b4-dfb0-4fb4-cf56-9288010f74c7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> <extra_id_0> they are great. Sentiment: Positive Review: Great plug. Easy to use and set up. Works great. Sentiment: Positive Review: Great plug. Sentiment: Positive Review: Great plug. Sentiment: Positive Review: Great plug. Sentiment: Positive Review: Great plug. Sentiment: Positive Review: Great plug. Sentiment: Positive Review: Great plug. Sentiment: Positive Review: Great plug. Sentiment:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the example:**\n",
        "\n",
        "- **Provided Reviews:** A set of reviews about smart plugs is given, each with a labeled sentiment (either Positive or Negative).\n",
        "\n",
        "**Few-Shot Examples:**\n",
        "\n",
        " - Review 1: Labeled as **Positive**\n",
        " - Review 2: Labeled as **Negative**\n",
        " - Review 3: Labeled as **Positive**\n",
        " - Review 4: Labeled as **Positive**\n",
        " - Review 5: Labeled as **Negative**\n",
        "\n",
        "These reviews serve as few-shot examples for the model. They illustrate how different sentiments are expressed in the reviews.\n",
        "\n",
        "**Prompt Structure:**\n",
        "\n",
        "The prompt includes these examples followed by a new review (Review 6) that the model needs to evaluate.\n",
        "\n",
        "The task is to predict the sentiment of the new review based on the patterns observed in the few-shot examples.\n",
        "\n",
        "**Model Execution:**\n",
        "\n",
        "The generate_response_from_prompt function takes this structured prompt and uses the T5 model to predict the sentiment of Review 6."
      ],
      "metadata": {
        "id": "VFm4-q1jNsa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of providing these few-shot examples is to guide the model in understanding the sentiment classification task. By presenting a few labeled examples, the model can learn the pattern of sentiment expression and apply this understanding to the new, unlabeled review. This method leverages the model's pre-trained knowledge and fine-tunes it with minimal additional data."
      ],
      "metadata": {
        "id": "7osFGGUGN6M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of FLAN-T5: Advancements Over T5 and Performance Insights"
      ],
      "metadata": {
        "id": "nL5ZtpyjJJfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **FLAN-T5 model** represents an advanced iteration of the original T5 model, **incorporating significant improvements in performance and versatility**. While T5 has set benchmarks in natural language processing, FLAN-T5 builds upon this foundation by addressing its limitations through extensive fine-tuning. Specifically, FLAN-T5 has been refined with additional training across over 1,000 diverse tasks and extended support for multiple languages, enhancing its capability to handle a broader range of applications.\n",
        "\n",
        "In recent evaluations, models such as Flan-PaLM 540B have demonstrated state-of-the-art performance, achieving impressive results like 75.2% on five-shot MMLU benchmarks. FLAN-T5, with its publicly available checkpoints, showcases strong few-shot learning abilities even when compared to much larger models like PaLM 62B. This performance highlights the effectiveness of instruction fine-tuning as a method to boost the performance and usability of pre-trained language models.\n",
        "\n",
        "This section aims to evaluate FLAN-T5's performance by examining its improvements over the base T5 model, particularly focusing on its handling of few-shot learning scenarios and its overall effectiveness in real-world tasks. We will delve into various benchmarks and practical applications to assess how FLAN-T5 stands up to its predecessors and its potential for advancing NLP capabilities."
      ],
      "metadata": {
        "id": "QPXPHCowJKCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"My name\"\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4G4pKR48Jxsf",
        "outputId": "e21063ca-1e9d-4274-9d21-42341bc1decf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Yvonne</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Today is\"\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NE92PhPIKBBX",
        "outputId": "dc225a73-e809-4259-f07c-231ec39fe159"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> a day of celebration.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What do you think of Mars?\"\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vTMDhqwIKFUm",
        "outputId": "dfa0aded-a52d-4756-dc09-5ac6d58ca765"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> good</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Summarization"
      ],
      "metadata": {
        "id": "4y2l8ZCzRI5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to condense a long piece of text into a shorter summary while preserving its essential meaning. For instance, you might use a prompt to ask the model to summarize a detailed description of climate change into a concise overview."
      ],
      "metadata": {
        "id": "33N_X1cHR2Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Climate change refers to significant and lasting changes in the Earth's climate \\\n",
        "and weather patterns over extended periods. It is primarily driven by human activities, \\\n",
        "especially the burning of fossil fuels such as coal, oil, and natural gas, which release \\\n",
        "greenhouse gases into the atmosphere. These gases, including carbon dioxide and methane, \\\n",
        "trap heat and lead to a warming of the planet. The consequences of climate change include \\\n",
        "rising sea levels, more frequent and severe extreme weather events, and disruptions to ecosystems \\\n",
        "and biodiversity. Addressing climate change requires global efforts to reduce greenhouse gas \\\n",
        "emissions, transition to renewable energy sources, and adapt to the changes that are already occurring.\"\"\"\n",
        "\n",
        "prompt = f\"Summarize: {text}\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "izlazBN0KTxo",
        "outputId": "8248cbd2-5904-4d2e-fbc1-c9afffdee0e3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Understand climate change.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification"
      ],
      "metadata": {
        "id": "gbm4qm1gR6Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique involves categorizing text into predefined classes based on its content. For example, you might use a prompt to classify the sentiment of a product review as positive or negative, guiding the model to understand and categorize emotional tones in the text."
      ],
      "metadata": {
        "id": "bXWyKmBtSACY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review1 = \"\"\"Love these plugs, have a few now. We use them to plug in lights and \\\n",
        "set timers to turn them on and off via a phone app. Easy to use and linked to \\\n",
        "the internet and apps. Good value for money.\"\"\"\n",
        "\n",
        "review2 = \"\"\"Tried and tried but could never get them to work right. Too bad \\\n",
        "I'm past my return date or they would have gone back.\"\"\"\n",
        "\n",
        "review3 = \"\"\"A well-sized, reliable smart plug. The app is easy to use and set \\\n",
        "up, and works well. I used them to make several lamps. Everything works fine - \\\n",
        "no problems.\"\"\"\n",
        "\n",
        "review4 = \"\"\"Great little product. Super easy to set up. Didn't even need to use \\\n",
        "the Alexa app to do so. Did it with my echo. Now I use it almost daily to turn on \\\n",
        "a light that was a pain to get to.\"\"\"\n",
        "\n",
        "review5 = \"\"\"If I could give this zero stars I would. Plug wouldn’t connect. I \\\n",
        "had to keep connecting it and finally just gave up and returned it. Customer service \\\n",
        "was a complete waste of time.\"\"\"\n",
        "\n",
        "review6 = \"\"\"The smart plug works as advertised. It’s a bit bulky but it does the job \\\n",
        "well. The setup was straightforward, and it integrates seamlessly with my home automation system.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Review: {review1}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review2}\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: {review3}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review4}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review5}\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: {review6}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NUC85exNK6c_",
        "outputId": "821f0d94-e0e4-4107-ae75-6dc15feba22a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Positive</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question Answering"
      ],
      "metadata": {
        "id": "_Wg7pFb6QXi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the objective is to extract specific information from a given context in response to a query. By providing the model with a piece of text and a question about it, the model generates a precise answer based on the provided information."
      ],
      "metadata": {
        "id": "xTJRTiwuSDza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Given the following context, answer the question:\n",
        "Context: The Amazon Rainforest is the largest tropical rainforest in the world. It spans several countries, including Brazil, Peru, and Colombia, and is home to a vast array of biodiversity. The rainforest plays a critical role in regulating the Earth's climate by absorbing carbon dioxide.\n",
        "Question: What role does the Amazon Rainforest play in regulating the Earth's climate?\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mLNj4QVVQV_Y",
        "outputId": "90713edb-a91c-45b4-e197-c09c8d53ad2b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> absorbing carbon dioxide</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation"
      ],
      "metadata": {
        "id": "JoYojyDUQiQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim is to convert text from one language to another while maintaining the original meaning. A prompt might ask the model to translate an English sentence into Spanish, demonstrating its capability to handle multilingual tasks."
      ],
      "metadata": {
        "id": "2p0MvOCCSGjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Translate the following English sentence to Spanish:\n",
        "Sentence: \"The quick brown fox jumps over the lazy dog.\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "87qpUf1mQuZR",
        "outputId": "5f882c13-1688-458f-db13-aebf500f1e33"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> El fox rojo amarillo jueve sobre el agujero lápidos.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creative Writing"
      ],
      "metadata": {
        "id": "9WQUbA1FQ0L_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the model generates imaginative and engaging text based on a given prompt. For example, you could ask the model to create a story starting with a specific scenario, allowing it to produce creative content that builds on the initial idea."
      ],
      "metadata": {
        "id": "DkvYmEU3SJC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Generate a creative story based on the following prompt:\n",
        "Prompt: \"Once upon a time, in a land where magic was real, a young girl discovered a hidden door in her grandmother's attic.\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8mY4eNOoQ2pI",
        "outputId": "67a59ce4-a8bf-4bd7-e163-f0679de4f7bd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<pad> A young girl discovered a hidden door in her grandmother's attic. She was a little girl. She was a little girl. She was a little girl. One day, she discovered a hidden door in her grandmother's attic. She was a little girl.</s>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Factual Information"
      ],
      "metadata": {
        "id": "oRpvFRPLQ9Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective here is to provide a clear and concise explanation of a factual concept. By prompting the model with a straightforward query about a scientific concept like black holes, it generates a brief and accurate explanation of the topic."
      ],
      "metadata": {
        "id": "uNyFl8AoSLmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Provide a brief explanation of what a black hole is:\n",
        "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. They are formed when massive stars collapse under their own gravity after their nuclear fuel is exhausted.\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-base\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4-XEREmkQ_7p",
        "outputId": "5ab767cf-6a07-41f7-82f2-dd58393190a4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Black holes are formed when massive stars collapse under their own gravity after their nuclear fuel is exhausted.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the FLAN-T5-Large Model: Performance Analysis and Capabilities"
      ],
      "metadata": {
        "id": "zJuNuXokLdPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **FLAN-T5-Large** model represents an advanced iteration of the T5 architecture, **designed to excel beyond its predecessors in both performance and versatility.** Leveraging a larger model size and additional fine-tuning on a diverse set of tasks and languages, **FLAN-T5-Large** is optimized to tackle more complex and specialized NLP challenges effectively.\n",
        "\n",
        "This section aims to evaluate the performance of the **FLAN-T5-Large** model across various natural language processing scenarios. We will assess how this model performs in tasks such as text generation, question answering, and machine translation, comparing its capabilities to those of previous models and its base version. With extensive fine-tuning on multiple tasks, **FLAN-T5-Large** promises enhanced performance in key benchmarks, making it a powerful tool for real-world applications.\n",
        "\n",
        "Throughout this evaluation, detailed results will be presented on how **FLAN-T5-Large** handles different NLP tasks, highlighting its strengths and any limitations. Comparative analysis with other models will provide insight into its effectiveness in practical applications and its impact on the field of machine learning."
      ],
      "metadata": {
        "id": "-lYfEHe_Lj6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"My name\"\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iyJInew6LuKP",
        "outputId": "9bdd7841-aad6-4738-ce9b-b1b25738a58d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> sarah</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Today is\"\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DNRhhlNFLuUI",
        "outputId": "1c3125af-7886-4d77-e18b-d50207cddf31"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Today, we celebrate the birth of the first woman to hold office in the United States.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What do you think of Mars?\"\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2UQYgcFnMOEg",
        "outputId": "544b7daf-109a-4ffa-e9b8-8e7a5b6d9c24"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> it is a planet</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Summarization"
      ],
      "metadata": {
        "id": "dBmzTc31SpmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to condense a long piece of text into a shorter summary while preserving its essential meaning. For instance, you might use a prompt to ask the model to summarize a detailed description of climate change into a concise overview."
      ],
      "metadata": {
        "id": "-ygYv4Y_StuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Climate change refers to significant and lasting changes in the Earth's climate \\\n",
        "and weather patterns over extended periods. It is primarily driven by human activities, \\\n",
        "especially the burning of fossil fuels such as coal, oil, and natural gas, which release \\\n",
        "greenhouse gases into the atmosphere. These gases, including carbon dioxide and methane, \\\n",
        "trap heat and lead to a warming of the planet. The consequences of climate change include \\\n",
        "rising sea levels, more frequent and severe extreme weather events, and disruptions to ecosystems \\\n",
        "and biodiversity. Addressing climate change requires global efforts to reduce greenhouse gas \\\n",
        "emissions, transition to renewable energy sources, and adapt to the changes that are already occurring.\"\"\"\n",
        "\n",
        "prompt = f\"Summarize: {text}\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xRC56-qnMUSw",
        "outputId": "1ba752f3-e529-4bb9-b59a-9ab52c982edb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Understand climate change.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Classification"
      ],
      "metadata": {
        "id": "jY1VNOOTSwxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique involves categorizing text into predefined classes based on its content. For example, you might use a prompt to classify the sentiment of a product review as positive or negative, guiding the model to understand and categorize emotional tones in the text."
      ],
      "metadata": {
        "id": "l-RjG0VnSz0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review1 = \"\"\"Love these plugs, have a few now. We use them to plug in lights and \\\n",
        "set timers to turn them on and off via a phone app. Easy to use and linked to \\\n",
        "the internet and apps. Good value for money.\"\"\"\n",
        "\n",
        "review2 = \"\"\"Tried and tried but could never get them to work right. Too bad \\\n",
        "I'm past my return date or they would have gone back.\"\"\"\n",
        "\n",
        "review3 = \"\"\"A well-sized, reliable smart plug. The app is easy to use and set \\\n",
        "up, and works well. I used them to make several lamps. Everything works fine - \\\n",
        "no problems.\"\"\"\n",
        "\n",
        "review4 = \"\"\"Great little product. Super easy to set up. Didn't even need to use \\\n",
        "the Alexa app to do so. Did it with my echo. Now I use it almost daily to turn on \\\n",
        "a light that was a pain to get to.\"\"\"\n",
        "\n",
        "review5 = \"\"\"If I could give this zero stars I would. Plug wouldn’t connect. I \\\n",
        "had to keep connecting it and finally just gave up and returned it. Customer service \\\n",
        "was a complete waste of time.\"\"\"\n",
        "\n",
        "review6 = \"\"\"The smart plug works as advertised. It’s a bit bulky but it does the job \\\n",
        "well. The setup was straightforward, and it integrates seamlessly with my home automation system.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Review: {review1}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review2}\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: {review3}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review4}\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: {review5}\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: {review6}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PI3hvrEZMZ8X",
        "outputId": "a70bea96-3bdf-4931-d60e-2d203be3b77f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Positive</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question Answering"
      ],
      "metadata": {
        "id": "IPFMZEcmS3PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the objective is to extract specific information from a given context in response to a query. By providing the model with a piece of text and a question about it, the model generates a precise answer based on the provided information."
      ],
      "metadata": {
        "id": "rozUy664S6Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Given the following context, answer the question:\n",
        "Context: The Amazon Rainforest is the largest tropical rainforest in the world. It spans several countries, including Brazil, Peru, and Colombia, and is home to a vast array of biodiversity. The rainforest plays a critical role in regulating the Earth's climate by absorbing carbon dioxide.\n",
        "Question: What role does the Amazon Rainforest play in regulating the Earth's climate?\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ibBBfQwyS9Ea",
        "outputId": "4b3ce8d6-a5e0-4f0d-c86c-582ff88ed2b9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> absorbing carbon dioxide</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation\n",
        "\n",
        "The aim is to convert text from one language to another while maintaining the original meaning. A prompt might ask the model to translate an English sentence into Spanish, demonstrating its capability to handle multilingual tasks."
      ],
      "metadata": {
        "id": "QAWaITPJTBGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Translate the following English sentence to Spanish:\n",
        "Sentence: \"The quick brown fox jumps over the lazy dog.\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "B86TwNOMTIAj",
        "outputId": "cb0ffd80-3b91-4416-ee80-7f5c936a0489"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> Sentence: \"El fox rojo rápido juega sobre el borracho.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creative Writing"
      ],
      "metadata": {
        "id": "Pz0xy5qhTLXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the model generates imaginative and engaging text based on a given prompt. For example, you could ask the model to create a story starting with a specific scenario, allowing it to produce creative content that builds on the initial idea."
      ],
      "metadata": {
        "id": "YvW-5JcZTN44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Generate a creative story based on the following prompt:\n",
        "Prompt: \"Once upon a time, in a land where magic was real, a young girl discovered a hidden door in her grandmother's attic.\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "DuJSV0gYTQbY",
        "outputId": "7aa6f381-5bdf-4572-f9f7-3de77a7cfc52"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<pad>The girl was a little girl who lived in a small town in the middle of nowhere. Her grandmother had a secret door in her attic that she had never seen before. She was so excited to find it, she hid it in her grandmother's attic. The next day, she found it in her grandmother's attic. She was so excited to find it, she hid it in her grandmother's attic. The next day, she found\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Factual Information"
      ],
      "metadata": {
        "id": "wNh-4i94TS84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective here is to provide a clear and concise explanation of a factual concept. By prompting the model with a straightforward query about a scientific concept like black holes, it generates a brief and accurate explanation of the topic."
      ],
      "metadata": {
        "id": "Px1obpOETVjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Provide a brief explanation of what a black hole is:\n",
        "A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it. They are formed when massive stars collapse under their own gravity after their nuclear fuel is exhausted.\"\"\"\n",
        "\n",
        "generate_response_from_prompt(\"google/flan-t5-large\", prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TLpYCx-oTXlo",
        "outputId": "30ad2187-eae2-4821-869b-05ba7dbf5a94"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad> A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}