{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb1c6ad5ce054fa580cb290cc1eb753d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_161c99eaa1de42fdb44a8735737c8440",
              "IPY_MODEL_6bf56b41600149ea9433ac30b23b0ba6",
              "IPY_MODEL_e82144ba7beb48c4bd1a3fe60f7721eb"
            ],
            "layout": "IPY_MODEL_46d59d65b60b412ea965d75d81d5169a"
          }
        },
        "161c99eaa1de42fdb44a8735737c8440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8813ae97f845e8b3257063f23632f5",
            "placeholder": "​",
            "style": "IPY_MODEL_dd43c546f77446c0a7b94f2f4eae641f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6bf56b41600149ea9433ac30b23b0ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6b5745d47d84882b64de3fe77feb2ed",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52858336d8e74e6784e0f249514f7117",
            "value": 8
          }
        },
        "e82144ba7beb48c4bd1a3fe60f7721eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d348e1e39fc4e20a906a229129a6017",
            "placeholder": "​",
            "style": "IPY_MODEL_2b4b18bf41ea4cc9b65206a884d3e322",
            "value": " 8/8 [00:52&lt;00:00,  8.79s/it]"
          }
        },
        "46d59d65b60b412ea965d75d81d5169a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8813ae97f845e8b3257063f23632f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd43c546f77446c0a7b94f2f4eae641f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6b5745d47d84882b64de3fe77feb2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52858336d8e74e6784e0f249514f7117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d348e1e39fc4e20a906a229129a6017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b4b18bf41ea4cc9b65206a884d3e322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Fine-Tuned Large Language Models for ILENIA Aguila7B and Latxa Projects"
      ],
      "metadata": {
        "id": "gSc5qTAiYPcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the field of linguistic resource development, **ILENIA** (*Impulsing Languages in Artificial Intelligence*) is part of the **Strategic Project for Economic Recovery and Transformation** (PERTE), within the framework of the **New Language Economy** (NEL). This project aims to boost Spain's new digital economy based on natural language, leveraging the potential of **Spanish** and other official languages as drivers of economic growth and international competitiveness in key areas such as **artificial intelligence**, **translation**, **education**, **cultural production and dissemination**, **research**, and **science**.\n",
        "\n",
        "ILENIA is a **collaborative initiative** that coordinates the development of **multilingual resources**, with a particular focus on **multilingual models** for text, speech, and machine translation. The goal is to address societal needs and align with contemporary technology, where **multilingualism** and **cross-lingual transfer** play a crucial role.\n",
        "\n",
        "The project spans **36 months** and is managed through a network comprising four centers that share **methodologies**, **objectives**, and **techniques**. The overall coordination is led by the **Barcelona Supercomputing Center-Centro Nacional de Supercomputación** (BSC-CNS).\n",
        "\n",
        "Within this framework, ILENIA works with **textual and speech data** sourced from various inputs. **Language models (LLMs)** are essential for creating new applications, enabling ongoing advancements in **natural language processing** in monolingual, multilingual, and multimodal contexts.\n",
        "\n",
        "The **generation and refinement** of LLMs is a progressive process that allows for **exponential growth** in model creation, optimizing the costs and resources required for training. This is crucial for developing **innovative and efficient solutions** in language processing.\n",
        "\n",
        "In this notebook, we will evaluate several LLMs using **fine-tuning techniques**, focusing on the **ILENIA Aguila7B** and **Latxa** projects. This analysis aims to assess the **performance** and **capabilities** of these models based on the project’s objectives, providing a detailed view of their effectiveness and applicability.\n",
        "\n",
        "Below, you will find links to the **Hugging Face** platform, which contains datasets, models, and AI resources developed to date within the ILENIA framework. These resources will serve as the basis for the evaluations conducted in this analysis."
      ],
      "metadata": {
        "id": "MiIjBQ2hX4HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of Ǎguila-7B"
      ],
      "metadata": {
        "id": "AfTsUJJiYV-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Description"
      ],
      "metadata": {
        "id": "nCc_izpRYtnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ǎguila-7B** is a transformer-based causal language model designed for Catalan, Spanish, and English. It is derived from the Falcon-7B model and has been trained on a trilingual corpus containing 26 billion tokens collected from publicly available corpora and web crawls.\n",
        "\n",
        "For more details, you can read the [Ǎguila-7B Hugging Face project description](https://huggingface.co/projecte-aina/aguila-7b)."
      ],
      "metadata": {
        "id": "I3BlENfiYzl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intended Uses and Limitations"
      ],
      "metadata": {
        "id": "AcElAjpSZBgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ǎguila-7B is primarily designed for causal language modeling tasks, specifically text generation. While it is ready-to-use for generating text, it is intended to be fine-tuned for various downstream tasks to enhance its performance for specific applications.\n"
      ],
      "metadata": {
        "id": "IwEDTE62ZD4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Use Ǎguila-7B"
      ],
      "metadata": {
        "id": "eb4TyRWSZG8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you're a writer looking for inspiration. You have a great opening sentence, but you need the model's help to continue your story. This is exactly what the **Ǎguila-7B model can do**. **By providing it with an initial text, the model generates a continuation, helping you craft coherent and engaging narratives**. In this example, we’ll walk through the process of using Ǎguila-7B to generate text, demonstrating its capabilities and providing insight into its performance."
      ],
      "metadata": {
        "id": "SxZdKd32bWAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the model, you can follow these steps:"
      ],
      "metadata": {
        "id": "f_0qom6cZJ2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Required Libraries"
      ],
      "metadata": {
        "id": "ADGe2P7gbCJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to bring in the essential tools for our task. We begin by importing the necessary libraries:"
      ],
      "metadata": {
        "id": "Lqgxa-2VbGcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "RFJDW4pgbHxC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what each import does:\n",
        "\n",
        "- **torch:** This is the core library for working with neural networks. It handles tensor computations and deep learning operations.\n",
        "- **pipeline:** A versatile utility from the transformers library that simplifies the use of pre-trained models for various tasks, including text generation.\n",
        "- **AutoTokenizer:** This class automatically fetches the tokenizer that matches our model. The tokenizer converts text into a format that the model understands.\n",
        "- **AutoModelForCausalLM:** This class loads the model architecture designed for causal language modeling, which is ideal for generating text."
      ],
      "metadata": {
        "id": "xA79oitFbKCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Input Text"
      ],
      "metadata": {
        "id": "pH_fFZVjbL5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up the initial text that we want the model to build upon:"
      ],
      "metadata": {
        "id": "h-NQHnpKbPAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"El mercat del barri és fantàstic, hi pots trobar\""
      ],
      "metadata": {
        "id": "zhI0awQwbQw2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, `input_text` is a sample sentence in Catalan. This is the starting point, and the model will generate additional text based on this input."
      ],
      "metadata": {
        "id": "U2_WpcIXbSkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Model and Tokenizer"
      ],
      "metadata": {
        "id": "21ALsxHKbqje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the text defined, we now load the model and tokenizer:"
      ],
      "metadata": {
        "id": "nNGtwIHNbs81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id  = \"projecte-aina/aguila-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "y1ZMDAEUbukl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **model_id:** Specifies which model we are using from the Hugging Face model hub.\n",
        "- **AutoTokenizer.from_pretrained(model_id):** Loads the tokenizer associated with the Ǎguila-7B model. The tokenizer breaks down our text into tokens that the model can process."
      ],
      "metadata": {
        "id": "_zxvBWePbwKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Text Generation Pipeline"
      ],
      "metadata": {
        "id": "xurMbreTb1Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then set up the pipeline that will handle the text generation:"
      ],
      "metadata": {
        "id": "KRQdcXGNb3Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "eb1c6ad5ce054fa580cb290cc1eb753d",
            "161c99eaa1de42fdb44a8735737c8440",
            "6bf56b41600149ea9433ac30b23b0ba6",
            "e82144ba7beb48c4bd1a3fe60f7721eb",
            "46d59d65b60b412ea965d75d81d5169a",
            "bd8813ae97f845e8b3257063f23632f5",
            "dd43c546f77446c0a7b94f2f4eae641f",
            "d6b5745d47d84882b64de3fe77feb2ed",
            "52858336d8e74e6784e0f249514f7117",
            "4d348e1e39fc4e20a906a229129a6017",
            "2b4b18bf41ea4cc9b65206a884d3e322"
          ]
        },
        "id": "j0Rj2T0SZN_e",
        "outputId": "6a620b2b-d95d-4371-e22c-7025b79f2e39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb1c6ad5ce054fa580cb290cc1eb753d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline is like a pre-configured tool for generating text. Here’s how we set it up:\n",
        "\n",
        "- **\"text-generation\":** Specifies that we are using the pipeline for generating text.\n",
        "- **model=model_id:** Tells the pipeline which model to use.\n",
        "- **tokenizer=tokenizer:** Provides the tokenizer that converts text into tokens.\n",
        "- **torch_dtype=torch.bfloat16:** Optimizes the computation by using a data type that balances performance and memory usage.\n",
        "- **trust_remote_code=True:** Assures that the model’s code is trusted.\n",
        "- **device_map=\"auto\":** Automatically selects the best hardware (CPU or GPU) for running the model."
      ],
      "metadata": {
        "id": "xJgrz2OicDK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Text"
      ],
      "metadata": {
        "id": "sp9AsKK3cE6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With everything set up, we can now generate text:"
      ],
      "metadata": {
        "id": "qF3_ktF8cGzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation = generator(\n",
        "    input_text,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "P5BcXKIkcJMR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what each parameter does:\n",
        "\n",
        "- **input_text:** The text that we want to expand.\n",
        "- **do_sample=True:** Allows the model to sample from the top predictions, introducing variety into the generated text.\n",
        "- **top_k=50:** Limits the sampling to the top 50 possible next tokens, ensuring more relevant and coherent outputs.\n",
        "- **eos_token_id=tokenizer.eos_token_id:** Marks the end of the generated sequence with the end-of-sequence token."
      ],
      "metadata": {
        "id": "7YVnVHa-cKrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the Result"
      ],
      "metadata": {
        "id": "9KrfTjOqcMne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we print the text generated by the model:"
      ],
      "metadata": {
        "id": "ge1W-SE6cOZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Result: {generation[0]['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kJtENELcP1K",
        "outputId": "01493bee-af4b-45f8-a5af-408d80200733"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: El mercat del barri és fantàstic, hi pots trobar de tot de tot tot, i tot el que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations and Bias"
      ],
      "metadata": {
        "id": "sZoBanj-ZWJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the time of submission, there were no specific measures taken to estimate bias and toxicity in the model. It is important to acknowledge that the model may exhibit biases, as it was trained on data collected from various web sources. Future research will address these issues, and updates will be made to this model card if necessary."
      ],
      "metadata": {
        "id": "91Rdi1wMZYk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language Adaptation"
      ],
      "metadata": {
        "id": "ehUzNQKsZZ_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Ǎguila-7B model was adapted from the original Falcon-7B model for Spanish and Catalan by swapping the tokenizer and adjusting the embedding layer."
      ],
      "metadata": {
        "id": "7XuDORFMZc0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Evaluation"
      ],
      "metadata": {
        "id": "yrTzZVzbZfQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the performance of Ǎguila-7B, we can use the provided example code to generate text based on a given input. This will help us assess the model's ability to generate coherent and contextually relevant text."
      ],
      "metadata": {
        "id": "KnWpJWYIZhYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"La tecnologia és fonamental en el desenvolupament de\"\n",
        "\n",
        "# Generate text using the model\n",
        "generation = generator(\n",
        "    input_text,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# Display the generated text\n",
        "print(f\"Generated Text: {generation[0]['generated_text']}\")"
      ],
      "metadata": {
        "id": "Y0P9AFL5Zivj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the model is expected to generate text that logically continues the input provided. Evaluating the relevance and coherence of the generated text will help determine the effectiveness of Ǎguila-7B for text generation tasks."
      ],
      "metadata": {
        "id": "ciYjC8BlZl8l"
      }
    }
  ]
}