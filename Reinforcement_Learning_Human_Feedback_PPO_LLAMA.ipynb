{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Case Study Introduction: Reinforcement Learning from Human Feedback with PPO on TinyLLAMA\n",
        "\n",
        "This case study explores the application of **Reinforcement Learning from Human Feedback (RLHF)** to enhance language models, focusing on reducing the generation of toxic or harmful content. The experiment will be conducted on **TinyLLAMA**, a lightweight version of the LLAMA model, leveraging human feedback to train the model to produce safer, more responsible outputs.\n",
        "\n",
        "#### Objectives:\n",
        "The primary goal of this study is to implement **RLHF** techniques to fine-tune TinyLLAMA, making it more adept at avoiding the generation of harmful, offensive, or toxic language. Specifically, the following objectives are outlined:\n",
        "1. **Content Moderation**: Improve the model’s ability to filter or avoid producing toxic, hate speech, or other undesirable outputs.\n",
        "2. **Ethical AI Development**: Ensure that the model’s outputs align with ethical standards, promoting responsible AI deployment.\n",
        "3. **Efficient Fine-Tuning**: Apply **Proximal Policy Optimization (PPO)** to optimize the model’s behavior based on feedback, balancing the complexity of the model and computational efficiency.\n",
        "4. **Evaluation with Reward Models**: Use a reward model, fine-tuned for detecting toxic content, to guide the reinforcement learning process.\n",
        "\n",
        "#### Methodology:\n",
        "To accomplish these objectives, **PPO**, a popular algorithm in reinforcement learning, will be employed. PPO allows for efficient optimization by adjusting the model’s outputs in small, controlled updates. This ensures stability during training and prevents drastic changes that could negatively affect the quality of the text generation.\n",
        "\n",
        "The **reward model** will be a fine-tuned version of **RoBERTa**, specifically designed for the detection of hate speech and toxic language. The version used, [facebook/roberta-hate-speech-dynabench-r4-target](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target), is a state-of-the-art model for identifying harmful content. It will serve as the evaluation metric during the RLHF process, rewarding the model when it generates safe, non-toxic text, and penalizing it when the outputs are deemed harmful.\n",
        "\n",
        "#### Use Cases:\n",
        "The techniques applied in this study have broad applications, including:\n",
        "- **Content Moderation Systems**: Enhancing automated moderation tools for social media platforms, forums, and other user-generated content sites.\n",
        "- **AI-Powered Assistants**: Ensuring conversational agents like chatbots or virtual assistants produce helpful, safe, and ethical responses in customer service, healthcare, or educational applications.\n",
        "- **Bias and Toxicity Mitigation**: Reducing bias, offensive language, or hate speech in text generation, contributing to more inclusive and respectful AI interactions.\n",
        "\n",
        "#### Why RLHF?\n",
        "**Reinforcement Learning from Human Feedback** is critical in this context because it allows the model to learn directly from human judgments, aligning its behavior with real-world expectations. Instead\n"
      ],
      "metadata": {
        "id": "rmmLRzAK0MJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_y_rPfky0MPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Installation Explanation\n",
        "\n",
        "In order to successfully implement the techniques discussed in this case study, several Python libraries and packages are required. The following dependencies are necessary for the project:\n"
      ],
      "metadata": {
        "id": "sEXQ4wvJ0MT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl xformers trl evaluate sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k62j54GX0MqF",
        "outputId": "28c2a51a-c324-4b0a-92d7-c73c7326185e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`accelerate`**:\n",
        "   - This library provides a simple interface to enable efficient training and inference of deep learning models across multiple devices (CPUs, GPUs). It optimizes the process of handling different hardware configurations and streamlines the setup for model training.\n",
        "\n",
        "2. **`peft`**:\n",
        "   - Stands for \"Parameter-Efficient Fine-Tuning.\" This library offers methods and tools for fine-tuning large pre-trained models efficiently, reducing the number of parameters that need to be updated during training. This is particularly useful in contexts where computational resources are limited.\n",
        "\n",
        "3. **`bitsandbytes`**:\n",
        "   - A library designed to facilitate the use of low-bit quantization methods for deep learning models. It allows models to be loaded and trained with reduced memory footprints (e.g., using 4-bit quantization), which is crucial for deploying large language models in resource-constrained environments.\n",
        "\n",
        "4. **`transformers`**:\n",
        "   - Developed by Hugging Face, this is one of the most widely used libraries for natural language processing. It provides access to a large variety of pre-trained models and tools for building and fine-tuning transformer-based architectures.\n",
        "\n",
        "5. **`trl`**:\n",
        "   - The \"Transformers Reinforcement Learning\" library is specifically designed to integrate reinforcement learning methods with transformer models. This library supports the implementation of techniques such as Proximal Policy Optimization (PPO), which is essential for the RLHF approach in this case study.\n",
        "\n",
        "6. **`xformers`**:\n",
        "   - A library focused on providing efficient and modular transformer architectures. It includes optimized implementations of transformer components that can improve performance and reduce memory consumption during model training and inference.\n",
        "\n",
        "7. **`evaluate`**:\n",
        "   - This library simplifies the process of evaluating models, particularly for natural language processing tasks. It provides easy access to various metrics and evaluation protocols that can be used to assess model performance, especially in the context of RLHF.\n",
        "\n",
        "8. **`sentencepiece`**:\n",
        "   - A text tokenizer and detokenizer mainly used for unsupervised text segmentation. It is essential for preparing input data for transformer models, allowing them to efficiently handle subword tokenization, which improves model performance on diverse linguistic inputs."
      ],
      "metadata": {
        "id": "8ymRi5NZ4sip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving and Configuring the Model and Tokenizer"
      ],
      "metadata": {
        "id": "lvzOLTZ160uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model and Tokenizer Download"
      ],
      "metadata": {
        "id": "y6-LyA-P68fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To optimize computational resource usage, particularly memory RAM, during the re-training and Reinforcement Learning processes, we will implement QLoRA on the model. This technique allows for efficient training while minimizing memory overhead, making it suitable for environments with limited computational capabilities. By applying QLoRA, we aim to enhance the model's performance while ensuring that resource consumption remains manageable."
      ],
      "metadata": {
        "id": "mDJUBLaC6_VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules from the transformers and torch libraries\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Configuring the BitsAndBytesConfig for optimized model loading and quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Load the model with 4-bit quantization to reduce memory usage\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Use the 'nf4' quantization type, which stands for NormalFloat 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # Set the compute precision to 16-bit floating point (fp16)\n",
        "    bnb_4bit_use_double_quant=False,  # Disable the use of double quantization (using an extra bit for accuracy)\n",
        ")"
      ],
      "metadata": {
        "id": "1WsN_Gwq0YfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we focus on the essential setup required to load a pre-trained causal language model effectively using the **transformers** and **torch** libraries. The goal is to configure the model for optimized performance, especially when dealing with large models that require significant computational resources.\n",
        "\n",
        "The first step involves importing the necessary modules. The **`AutoModelForCausalLM`** class from the **transformers** library serves as a gateway to various pre-trained language models, allowing us to leverage the capabilities of state-of-the-art architectures for text generation tasks. Coupled with this is the **`BitsAndBytesConfig`**, which plays a critical role in optimizing the loading process through quantization techniques.\n",
        "\n",
        "Quantization is a method that reduces the precision of the model's weights and activations, thus decreasing the overall memory usage without severely impacting performance. By setting **`load_in_4bit=True`**, we enable the model to load using a 4-bit quantization scheme, which significantly cuts down on the memory requirements. This is particularly valuable when working with large models, making them more feasible to deploy in environments with limited resources.\n",
        "\n",
        "The choice of the quantization type, specified as **`\"nf4\"`** (NormalFloat 4-bit), reflects a thoughtful balance between efficiency and performance. This quantization method aims to preserve as much of the model's predictive capabilities as possible while still achieving substantial memory savings.\n",
        "\n",
        "Furthermore, by setting the **`bnb_4bit_compute_dtype`** to **`torch.float16`**, we are opting for 16-bit floating-point precision during computations. This decision enhances processing speed and reduces memory consumption, facilitating faster inference times and more efficient training cycles.\n",
        "\n",
        "Finally, the configuration includes the option **`bnb_4bit_use_double_quant=False`**, which simplifies the quantization process by disabling double quantization. This choice aligns with the goal of maintaining a streamlined and efficient loading mechanism."
      ],
      "metadata": {
        "id": "E5gGnP6a7GKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre del modelo\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "\n",
        "# Leemos el modelo pre-entrenado el modelo LLAMA2-7b-chat\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    low_cpu_mem_usage=True # Reduccion del consumo de cpu y memoria al leer el modelo\n",
        ")\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002"
      ],
      "metadata": {
        "id": "bodT1zfc0d2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this segment, we set the stage for utilizing a pre-trained language model specifically designed for conversational applications. The model we will be working with is identified as **`\"PY007/TinyLlama-1.1B-Chat-v0.3\"`**, a lightweight variant of the LLAMA architecture optimized for chat-based interactions. The choice of this model reflects a focus on generating contextually relevant and engaging responses, which is critical for applications involving human-computer dialogue.\n",
        "\n",
        "The first step involves loading the pre-trained model using the **`AutoModelForCausalLM`** class from the **transformers** library. This class provides a seamless way to access and leverage various pre-trained language models. By invoking the **`from_pretrained`** method, we can load the model directly from its designated repository, making it convenient to incorporate state-of-the-art natural language processing capabilities into our project.\n",
        "\n",
        "To ensure that the model operates efficiently, we configure several parameters during the loading process. The **`quantization_config`** parameter is set to **`bnb_config`**, which we previously defined. This configuration allows the model to utilize 4-bit quantization, optimizing memory usage and making it feasible to deploy on hardware with limited resources.\n",
        "\n",
        "Additionally, the **`device_map`** is specified as **`{\"\": 0}`**, indicating that the model will be loaded onto the first available device, typically the GPU. This configuration helps to accelerate computations and enhance the model's performance during inference.\n",
        "\n",
        "Another important aspect of the loading process is the **`low_cpu_mem_usage`** parameter, set to **`True`**. By enabling this option, we aim to reduce CPU and memory consumption when loading the model. This feature is particularly beneficial when working with large models, as it helps mitigate resource contention and ensures smoother operation during the execution of tasks.\n",
        "\n",
        "Finally, the variable **`CHAT_EOS_TOKEN_ID`** is assigned the value **`32002`**. This token ID represents the end-of-sequence marker for the chat model, allowing the system to recognize when a response has concluded. Identifying the end of a generated response is crucial for maintaining coherent and contextually appropriate conversations.\n"
      ],
      "metadata": {
        "id": "pj8hhUtz6idv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "nxf8OL-a3H4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Explanation:\n",
        "1. **Importing the Tokenizer**:\n",
        "   - `AutoTokenizer`: This class from the `transformers` library is used to load a pre-trained tokenizer, which is responsible for converting text into tokens that the model can process.\n",
        "\n",
        "2. **Tokenizer Loading**:\n",
        "   - The tokenizer is loaded using `AutoTokenizer.from_pretrained()`.\n",
        "   - **Parameters**:\n",
        "     - `model_name`: Specifies the model name (`\"PY007/TinyLlama-1.1B-Chat-v0.3\"`) to ensure the tokenizer matches the model architecture.\n",
        "     - `trust_remote_code=True`: This option allows loading custom or external tokenizer implementations that might be hosted with the model, ensuring compatibility with the specific model version.\n"
      ],
      "metadata": {
        "id": "aEqkA9MC3IlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation Using the Model"
      ],
      "metadata": {
        "id": "49b0QfBd6lj3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Keyck-H6lt2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}